# Base config

# Mandatory experiment name; used in output path and logging
experiment:
  name: "???"
  seed: 42

# Number of iterative rounds: generate -> train -> regenerate -> train ...
rounds_of_training: 2

# System configuration
system:
  dtype: bfloat16
  amp_dtype: bfloat16
  attn_impl: eager  # Use 'eager' (stable) or 'flash_attention_2' (faster, requires installation)
  clear_cache_interval: 10  # Clear cache every N steps to prevent OOM

# Generation hyperparameters
generation:
  max_new_tokens: 2048
  temperature: 0.8
  top_p: 0.9
  use_chat_template: true

# Training hyperparameters
training:
  micro_batch_size: 64  # Micro-batching for gradient accumulation (helps with memory)
  gradient_checkpointing: true  # Enable to save memory at cost of speed
  # Which datasets to train on
  datasets:
    - GSM8K
    # - TLDR
    - IMDBGen

# Data collection hyperparameters
data_collection:
  samples_per_example: 4
  seed: 42
  max_examples: 16
  reward_batch_size: 32  # Increased from 2 to 32 for better GPU utilization
  bradley_terry_sampling: true
  bradley_terry_beta: 1.0

# Evaluation configuration
evaluation:
  # Evaluate guided decoding at multiple eta values
  etas: [0.25, 0.5, 1.0, 2.0, 4.0, 8.0]
  # Number of samples for majority voting evaluation
  num_samples: 16
  # Batch size for evaluation generation
  batch_size: 64
  # For each training dataset, which datasets to evaluate on
  datasets_by_train:
    GSM8K:
      - GSM8K
      - MATH
      - AIME
    TLDR:
      - TLDR
    IMDBGen:
      - IMDBGen

# Algorithms (present = enabled). Each entry holds algo-specific hyperparams
# Parallel execution settings
parallel_generation: false  # Disable multi-GPU parallel generation (each job gets one GPU)
use_parallel_execution: true  # Enable parallel job execution

algos:
  PITA:
    epochs: 10
    batch_size: 64  
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4  
    lr: 2.0e-5
    weight_decay: 0.0
    grad_clip: 1.0
    loss_type: bce
    num_atoms: 11
    V_min: 0.0
    V_max: 1.0
    guidance:
      eta: 1.0
      mode: bernoulli
      top_k: 20
      use_cache: false
  QSharp:
    epochs: 10
    batch_size: 64  
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4  
    lr: 2.0e-5
    weight_decay: 0.0
    grad_clip: 1.0
    loss_type: bce
    num_atoms: 11
    V_min: 0.0
    V_max: 1.0
    guidance:
      eta: 1.0
      mode: bernoulli
      top_k: 20
      use_cache: false
  QSharp-HF:
    epochs: 10              # ValueClassifier training epochs (Phase 3)
    proxy_epochs: 5        # Proxy RM training epochs (Phase 1)
    batch_size: 64
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4
    lr: 2.0e-5             # ValueClassifier training LR
    proxy_lr: 2.0e-5       # Proxy RM training LR
    weight_decay: 0.0
    grad_clip: 1.0
    proxy_loss_type: bradley_terry  # Proxy RM trained with preference loss
    loss_type: mle         # ValueClassifier trained on proxy rewards
    num_atoms: 11
    V_min: 0.0
    V_max: 1.0
    guidance:
      eta: 1.0
      mode: expectation
      top_k: 20
      use_cache: false
  DPO:
    beta: 0.1
    lr: 2.0e-5
    weight_decay: 0.0
    epochs: 5
    grad_clip: 1.0
    batch_size: 64  
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4  
  GRPO:
    samples_per_prompt: 4
    kl_coef: 0.05
    lr: 2.0e-5
    weight_decay: 0.0
    epochs: 5
    grad_clip: 1.0
    batch_size: 64 
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4  
  GRPO-HF:
    samples_per_prompt: 4
    kl_coef: 0.05
    lr: 2.0e-5              # GRPO policy training LR
    proxy_lr: 2.0e-5        # Proxy RM training LR
    weight_decay: 0.0
    epochs: 10               # GRPO policy training epochs
    proxy_epochs: 5         # Proxy RM training epochs
    grad_clip: 1.0
    batch_size: 64
    max_batch_num_tokens: 16384  # Dynamic batching: adjust batch size based on sequence length
    num_workers: 4
    proxy_loss_type: bradley_terry  # Proxy RM trained with preference loss
    num_atoms: 11
    V_min: 0.0
    V_max: 1.0

# Model pairs; map of ref -> classifier
model_pairs:
  - gemma
  - llama
  # - phi
  # - gpt

# Datasets (present = enabled). Each entry holds dataset-specific hyperparams
datasets:
  AIME:
    train_hf_config: AIME2025-I
    train_split: test
    question_key: question
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: AIME2025-I
    test_split: test
  GSM8K:
    train_hf_config: main
    train_split: train
    question_key: question
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: main
    test_split: test
  TLDR:
    train_hf_config: ""
    train_split: train
    question_key: prompt
    answer_key: completion
    reward_model: OpenAssistant/reward-model-deberta-v3-large-v2
    test_hf_config: ""
    test_split: test
  IMDBGen:
    train_hf_config: ""
    train_split: train
    question_key: text
    answer_key: label
    reward_model: lvwerra/distilbert-imdb
    test_hf_config: ""
    test_split: test
  MATH:
    train_hf_config: ""
    train_split: test
    question_key: problem
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: ""
    test_split: test

# Hydra output directory
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${experiment.name}-${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${experiment.name}-${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  # Disable Hydra's automatic logging since we use our own logger
  job_logging:
    disable_existing_loggers: false
    handlers:
      file:
        # Redirect to same file as our manual logger
        filename: ${hydra.runtime.output_dir}/run.log
