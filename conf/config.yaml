# Base config

# Mandatory experiment name; used in output path and logging
experiment:
  name: "???"
  seed: 42

# Global/common hyperparams
common:
  max_new_tokens: 256
  temperature: 0.8
  top_p: 0.9
  dtype: bfloat16
  use_chat_template: true
  bt_sampling: true

# Data collection hyperparams
collection:
  samples_per_example: 4
  seed: 47
  max_examples: 1

# Algorithms (present = enabled). Each entry holds algo-specific hyperparams
algos:
  # PITA:
  #   bt_beta: 1.0
  # Q#: {}
  # Q#HF: {}
  DPO:
    bt_beta: 1.0
    beta: 0.1
    lr: 2.0e-5
    weight_decay: 0.0
    epochs: 5
    grad_clip: 1.0
    batch_size: 2
    num_workers: 2
  # RLHF:
  #   bt_beta: 1.0
  # BEST_OF_N: {}

# Model pairs; map of ref -> classifier
model_pairs:
  # - gemma
  # - llama
  - gpt

# Datasets (present = enabled). Each entry holds dataset-specific hyperparams
datasets:
  AIME:
    hf_config: AIME2025-I
    split: test
    question_key: question
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
  # GSM8K:
  #   hf_config: main
  #   split: train
  #   question_key: question
  #   answer_key: answer
  #   reward_model: weqweasdas/RM-Gemma-2B
  # MATH:
  #   hf_config: ""
  #   split: test
  #   question_key: problem
  #   answer_key: answer
  #   reward_model: weqweasdas/RM-Gemma-2B

# Hydra output directory
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${experiment.name}-${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${experiment.name}-${now:%H-%M-%S}
    subdir: ${hydra.job.num}

 
