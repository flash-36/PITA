experiment:
  name: base_model_cot8_evaluation
  seed: 42
rounds_of_training: 0
system:
  dtype: bfloat16
  amp_dtype: bfloat16
  attn_impl: sdpa
generation:
  max_new_tokens: 1024
  temperature: 0.8
  top_p: 0.9
  use_chat_template: true
training:
  gradient_checkpointing: true
  datasets:
  - GSM8K
data_collection:
  max_examples: 250
  reward_batch_size: 32
evaluation:
  num_samples: 8
  batch_size: 16
  max_examples: 250
  kl_sample_ratio: 0.1
  datasets_by_train:
    GSM8K:
    - GSM8K
    - MATH
    - AIME
model_pairs:
- gemma
- llama
- phi
algos: {}  # No training jobs
datasets:
  GSM8K:
    train_hf_config: main
    train_split: train
    question_key: question
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: main
    test_split: test
  MATH:
    train_hf_config: ''
    train_split: test
    question_key: problem
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: ''
    test_split: test
  AIME:
    train_hf_config: AIME2025-I
    train_split: test
    question_key: question
    answer_key: answer
    reward_model: weqweasdas/RM-Gemma-2B
    test_hf_config: AIME2025-I
    test_split: test
